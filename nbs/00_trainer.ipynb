{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import jax, jax.numpy as jnp, jax.random as jrand\n",
    "import haiku as hk\n",
    "import optax\n",
    "import chex\n",
    "from dataclasses import dataclass\n",
    "import functools as ft\n",
    "from typing import Callable, Tuple, Any, Sequence, Iterable, Mapping, Dict, List, NamedTuple\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainState(NamedTuple):\n",
    "    epoch: int\n",
    "    step: int\n",
    "    params: hk.Params\n",
    "    state: hk.State\n",
    "    opt_state: optax.OptState\n",
    "    next_key: jrand.PRNGKey\n",
    "    logs: dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Trainer:\n",
    "    transformed: hk.TransformedWithState | hk.MultiTransformedWithState\n",
    "    optimizers: optax.GradientTransformation | Sequence[optax.GradientTransformation]\n",
    "    rng_key: jrand.PRNGKey = None\n",
    "\n",
    "    # callback functions\n",
    "    callbacks: List[Any] = None\n",
    "    train_step_fn: TrainStepFn = None\n",
    "    val_step_fn: ValStepFn = None\n",
    "\n",
    "    # trainer configs\n",
    "    lr: float = 1e-3\n",
    "    n_epochs: int = 1\n",
    "\n",
    "    # model train state\n",
    "    _train_state: TrainState = None\n",
    "\n",
    "    @property\n",
    "    def train_state(self):\n",
    "        return self._train_state\n",
    "\n",
    "    def _initialize_key(self):\n",
    "        if self.rng_key is None:    return jrand.PRNGKey(42) # TODO: use global\n",
    "        else:                       return self.rng_key\n",
    "\n",
    "    def _initialize_model_param_and_state(\n",
    "        self, key: jrand.PRNGKey, shape: Sequence[int]\n",
    "    ):\n",
    "        params, state = self.transformed.init(key, jnp.zeros(shape))\n",
    "        return params, state\n",
    "\n",
    "    def _initialize_opt_state(self, params: hk.Params):\n",
    "        # if isinstance(self.optimizers, Sequence):\n",
    "        #     return [opt.init(params) for opt in self.optimizers]\n",
    "        if isinstance(self.optimizers, optax.GradientTransformation):\n",
    "            return self.optimizers.init(params) \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid optimizers. Expected `optax` optimizers.\")\n",
    "    \n",
    "    # def _initialize_step_fns(self):\n",
    "    #     if self.train_step_fn is None:\n",
    "    #         self.train_step_fn = TrainStepFn(self)\n",
    "\n",
    "    #     if self.val_step_fn is None:\n",
    "    #         self.val_step_fn = ValStepFn(self)\n",
    "\n",
    "    def _initialize(self, batch: Tuple[jax.Array, ...]):\n",
    "        xs_shape = batch[0].shape\n",
    "        key1, next_key = jrand.split(self._initialize_key())\n",
    "        params, state = self._initialize_model_param_and_state(key1, shape=xs_shape)\n",
    "        opt_states = self._initialize_opt_state(params)\n",
    "        self._train_state = TrainState(\n",
    "            epoch=0, step=0,\n",
    "            params=params, state=state, \n",
    "            opt_state=opt_states,\n",
    "            next_key=next_key,\n",
    "        )\n",
    "        # self._initialize_step_fns()\n",
    "        \n",
    "    def _run_callbacks(self):\n",
    "        if self.callbacks is not None:\n",
    "            for callback in self.callbacks:\n",
    "                callback(self.train_state)\n",
    "\n",
    "    def update_train_state(self, train_state: TrainState = None, **kwargs):\n",
    "        if train_state is None and kwargs == {}:\n",
    "            raise ValueError(\"Either `train_state` or `kwargs` must be provided.\")\n",
    "        if train_state is None:\n",
    "            train_state = self.train_state._replace(**kwargs)\n",
    "        self._train_state = train_state\n",
    "\n",
    "    def train_step(self, batch: Tuple[jax.Array, ...]) -> None:\n",
    "        if self.train_step_fn is None:\n",
    "            self.train_step_fn = TrainStepFn(self)\n",
    "        upt_train_state = self.train_step_fn(self.train_state, batch)\n",
    "        self.update_train_state(upt_train_state)\n",
    "    \n",
    "    def val_step(self, batch: Tuple[jax.Array, ...]) -> None:\n",
    "        if self.val_step_fn is None:\n",
    "            self.val_step_fn = ValStepFn(self)\n",
    "        upt_train_state = self.val_step_fn(self.train_state, batch)\n",
    "        self.update_train_state(upt_train_state)\n",
    "\n",
    "    def validate_train_step(self, batch: Tuple[jax.Array, ...]):\n",
    "        cur_train_state = copy.deepcopy(self.train_state)\n",
    "        self.train_step(batch)\n",
    "        if cur_train_state.step == self.train_state.step:\n",
    "            raise ValueError(\"Train state is not updated after `train_step`.\")\n",
    "\n",
    "    def fit(self, train_dataloader, val_dataloader=None):\n",
    "        cur_steps = 0\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for batch in train_dataloader:\n",
    "                if self.train_state is None:\n",
    "                    self._initialize(batch)\n",
    "                    self.validate_train_step(batch)\n",
    "                else:\n",
    "                    self.train_step(batch)\n",
    "                self._run_callbacks()\n",
    "                cur_steps += 1\n",
    "                assert cur_steps == self.train_state.step\n",
    "\n",
    "            if val_dataloader is not None:\n",
    "                for batch in val_dataloader:\n",
    "                    self.val_step(self.train_state, batch)\n",
    "                    cur_steps += 1\n",
    "                    assert cur_steps == self.train_state.step\n",
    "\n",
    "            self.update_train_state(epoch=epoch+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StepFn:\n",
    "    def __init__(self, trainer: Trainer, *args, **kwargs) -> None:\n",
    "        self._trainer = trainer\n",
    "\n",
    "    @property\n",
    "    def trainer(self): return self._trainer\n",
    "\n",
    "    @property\n",
    "    def forward(self): return self.trainer.transformed\n",
    "    \n",
    "    @property\n",
    "    def optimizer(self): return self.trainer.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainStepFn(StepFn):\n",
    "    loss_fn: Callable[[jax.Array, jax.Array], float] = None\n",
    "    \n",
    "    def __init__(self, trainer: Trainer, loss_fn=None) -> None:\n",
    "        super().__init__(trainer)\n",
    "        if loss_fn is None:\n",
    "            loss_fn = optax.softmax_cross_entropy_with_integer_labels\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    @ft.partial(jax.jit, static_argnums=(0,))\n",
    "    def __call__(self, train_state: TrainState, batch: Tuple[jax.Array, ...]):\n",
    "        def loss_fn(params: hk.Params):\n",
    "            logits, new_state = self.forward.apply(\n",
    "                params, state,\n",
    "                rng_key, # <== rng\n",
    "                inputs, is_training=True # <== inputs\n",
    "            )\n",
    "            loss = self.loss_fn(logits, labels).mean()\n",
    "            return (loss, new_state)\n",
    "        \n",
    "        inputs, labels = batch\n",
    "        rng_key, next_key = jrand.split(train_state.next_key)\n",
    "        state = train_state.state\n",
    "        (loss, new_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(train_state.params)\n",
    "        updates, new_opt_state = self.optimizer.update(\n",
    "            grads, train_state.opt_state, train_state.params)\n",
    "        new_params = optax.apply_updates(train_state.params, updates)\n",
    "        return TrainState(\n",
    "            epoch=train_state.epoch,\n",
    "            step=train_state.step + 1,\n",
    "            params=new_params,\n",
    "            state=new_state,\n",
    "            opt_state=new_opt_state,\n",
    "            next_key=next_key,\n",
    "            logs={'train/loss': loss}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ValStepFn(StepFn):\n",
    "    metric_fns: Dict[str, Callable[[jax.Array, jax.Array], float]] = None\n",
    "\n",
    "    def __init__(self, trainer: Trainer, metric_fns=None) -> None:\n",
    "        super().__init__(trainer)\n",
    "        if metric_fns is None:\n",
    "            metric_fns = optax.softmax_cross_entropy_with_integer_labels\n",
    "        self.metric_fns = metric_fns\n",
    "\n",
    "    @ft.partial(jax.jit, static_argnums=(0,))\n",
    "    def __call__(self, train_state: TrainState, batch: Tuple[jax.Array, ...]):        \n",
    "        inputs, labels = batch\n",
    "        rng_key, next_key = jrand.split(train_state.next_key)\n",
    "        logits, _ = self.forward.apply(\n",
    "            train_state.params, train_state.state,\n",
    "            rng_key, # <== rng\n",
    "            inputs, is_training=False # <== inputs\n",
    "        )\n",
    "        logs = {\n",
    "            f'val/{name}': metric_fn(logits, labels)\n",
    "            for name, metric_fn in self.metric_fns.items()\n",
    "        }\n",
    "\n",
    "        train_state = train_state._replace(\n",
    "            step=train_state.step + 1,\n",
    "            next_key=next_key, logs=logs\n",
    "        )\n",
    "        return train_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBatchNorm(hk.Module):\n",
    "    \"Linear layer with batch normalization\"\n",
    "    def __init__(self, output_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, x, training=False):\n",
    "        return hk.BatchNorm(True, True, 0.9)(hk.Linear(self.output_size)(x), is_training=training)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hk_module(\n",
    "    module: hk.Module, # haiku module \n",
    "    *args, # haiku module arguments\n",
    "    **kargs, # haiku module arguments\n",
    ") -> hk.Transformed:\n",
    "\n",
    "    def model_fn(x, is_training: bool = True):\n",
    "        return module(*args, **kargs)(x, is_training)\n",
    "    \n",
    "    return hk.transform_with_state(model_fn)\n",
    "\n",
    "module = make_hk_module(LinearBatchNorm, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 15:08:49.637593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from jax_dataloader import DataLoader, ArrayDataset\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = make_classification(n_samples=2000, n_features=10, random_state=0)\n",
    "ds = ArrayDataset(xs, ys)\n",
    "dl = DataLoader(ds, 'jax', batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    transformed=module,\n",
    "    optimizers=optax.adam(1e-3),\n",
    "    callbacks=[],\n",
    ")\n",
    "# model._initialize_param_and_state(jrand.PRNGKey(42), (10, 10))\n",
    "# model._initialize_opt_state(jrand.PRNGKey(42))\n",
    "# trainer._initialize((jnp.zeros((10, 10)), jnp.zeros((10, 10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-dataloader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
