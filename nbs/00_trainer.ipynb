{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import jax, jax.numpy as jnp, jax.random as jrand\n",
    "import haiku as hk\n",
    "import optax\n",
    "import chex\n",
    "from dataclasses import dataclass\n",
    "import functools as ft\n",
    "from typing import Callable, Tuple, Any, Sequence, Iterable, Mapping, Dict, List, NamedTuple\n",
    "import copy\n",
    "from haiku_trainer.callbacks import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainState(NamedTuple):\n",
    "    epoch: int\n",
    "    step: int\n",
    "    params: hk.Params\n",
    "    state: hk.State\n",
    "    opt_state: optax.OptState\n",
    "    next_key: jrand.PRNGKey\n",
    "    logs: dict = None\n",
    "\n",
    "    def __eq__(self, compare: TrainState) -> bool:\n",
    "        return (self.epoch == compare.epoch) and (self.step == compare.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Trainer:\n",
    "    transformed: hk.TransformedWithState | hk.MultiTransformedWithState\n",
    "    optimizers: optax.GradientTransformation | Sequence[optax.GradientTransformation]\n",
    "    rng_key: jrand.PRNGKey = None\n",
    "\n",
    "    # callback functions\n",
    "    callbacks: Sequence[Callback] = None\n",
    "    step_fn: StepFn = None\n",
    "\n",
    "    # trainer configs\n",
    "    lr: float = 1e-3\n",
    "    n_epochs: int = 1\n",
    "\n",
    "    # model train state\n",
    "    _train_state: TrainState = None\n",
    "\n",
    "    @property\n",
    "    def train_state(self):\n",
    "        return self._train_state\n",
    "\n",
    "    def _initialize_key(self):\n",
    "        if self.rng_key is None:    return jrand.PRNGKey(42) # TODO: use global\n",
    "        else:                       return self.rng_key\n",
    "\n",
    "    def _initialize_callbacks(self):\n",
    "        if self.callbacks is None:\n",
    "            self.callbacks = CallbackList()\n",
    "        elif isinstance(self.callbacks, CallbackList):\n",
    "            self.callbacks = self.callbacks\n",
    "        elif isinstance(self.callbacks, Sequence):\n",
    "            self.callbacks = CallbackList(self.callbacks)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid callbacks. Expected `CallbackList` or `Sequence[Callback]`.\")\n",
    "\n",
    "        self.callbacks.init_trainer(self)\n",
    "\n",
    "    def _initialize_step_fn(self):\n",
    "        if self.step_fn is None:\n",
    "            self.step_fn = DefaultStepFn(trainer=self)\n",
    "        else:\n",
    "            if isinstance(self.step_fn, StepFn):\n",
    "                self.step_fn.init_trainer(self)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid `Trainer.step_fn`. Expected `StepFn`, but got `{type(self.step_fn)}`.\")\n",
    "    \n",
    "    def _initialize(self):\n",
    "        self._initialize_callbacks()\n",
    "        self._initialize_step_fn()\n",
    "        \n",
    "    def _run_callbacks(self, hook_name: str):\n",
    "        hook_fn = getattr(self.callbacks, hook_name, None)\n",
    "        if hook_fn is not None:\n",
    "            hook_fn(self.train_state)\n",
    "\n",
    "    def _run_step_fn(self, step_name: str, batch: Tuple[jax.Array, ...], validate: bool = False):\n",
    "        step_fn = getattr(self.step_fn, step_name)\n",
    "        train_state = step_fn(self.train_state, batch)\n",
    "\n",
    "        if validate and train_state == self.train_state:\n",
    "            raise ValueError(f\"Train state is not updated after `{step_name}`.\")\n",
    "        self.update_train_state(train_state)\n",
    "\n",
    "    def update_train_state(self, train_state: TrainState = None, **kwargs):\n",
    "        if train_state is None and kwargs == {}:\n",
    "            raise ValueError(\"Either `train_state` or `kwargs` must be provided.\")\n",
    "        if train_state is None:\n",
    "            train_state = self.train_state._replace(**kwargs)\n",
    "        self._train_state = train_state\n",
    "\n",
    "    def fit(self, train_dataloader, val_dataloader=None):\n",
    "        self._initialize()\n",
    "        self._run_callbacks(\"on_train_begin\")\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self._run_callbacks(\"on_epoch_begin\")\n",
    "            for batch in train_dataloader:\n",
    "                self._run_callbacks(\"on_train_batch_begin\")\n",
    "                if self.train_state is None:\n",
    "                    self._run_step_fn(\"init_step\", batch)\n",
    "                self._run_step_fn(\"train_step\", batch)\n",
    "                self._run_callbacks(\"on_train_batch_end\")\n",
    "            self._run_callbacks(\"on_epoch_end\")\n",
    "\n",
    "            if val_dataloader is not None:\n",
    "                self._run_callbacks(\"on_val_begin\")\n",
    "                for batch in val_dataloader:\n",
    "                    self._run_callbacks(\"on_val_batch_begin\")\n",
    "                    self._run_step_fn(\"val_step\", batch)\n",
    "                    self._run_callbacks(\"on_val_batch_end\")\n",
    "                self._run_callbacks(\"on_val_end\")\n",
    "\n",
    "            self._run_callbacks(\"on_train_end\")\n",
    "            self._run_step_fn(\"epoch_step\", batch=None)\n",
    "        \n",
    "        self._run_callbacks(\"on_train_end\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StepFn:\n",
    "    def __init__(self, trainer: Trainer=None, *args, **kwargs) -> None:\n",
    "        if trainer is not None:\n",
    "            self.init_trainer(trainer)\n",
    "\n",
    "    def init_trainer(self, trainer: Trainer):\n",
    "        self._trainer = trainer\n",
    "\n",
    "    @property\n",
    "    def trainer(self): return self._trainer\n",
    "\n",
    "    @property\n",
    "    def transformed(self): return self.trainer.transformed\n",
    "\n",
    "    forward = transformed\n",
    "    \n",
    "    @property\n",
    "    def optimizers(self): return self.trainer.optimizers\n",
    "\n",
    "    def init_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        key1, next_key = jrand.split(self._init_key())\n",
    "        \n",
    "        params, state = self._init_params_and_state(key1, batch[0])\n",
    "        opt_states = self._init_opt_state(params)\n",
    "        return TrainState(\n",
    "            epoch=0, step=0, params=params, state=state, \n",
    "            opt_state=opt_states, next_key=next_key,\n",
    "        )\n",
    "    \n",
    "    def epoch_step(self, train_state: TrainState, batch=None) -> TrainState:\n",
    "        return train_state._replace(epoch=train_state.epoch+1)\n",
    "    \n",
    "    def train_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def val_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _init_key(self):\n",
    "        if self.trainer.rng_key is None:\n",
    "            return jrand.PRNGKey(0)\n",
    "        elif isinstance(self.trainer.rng_key, jrand.PRNGKey):\n",
    "            return self.trainer.rng_key\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid rng_key. Expected `jax.random.PRNGKey`.\")\n",
    "\n",
    "    def _init_params_and_state(self, key: jrand.PRNGKey, xs: jax.Array):\n",
    "        params, state = self.transformed.init(key, xs)\n",
    "        return params, state\n",
    "\n",
    "    def _init_opt_state(self, params: hk.Params):\n",
    "        if isinstance(self.optimizers, optax.GradientTransformation):\n",
    "            return self.optimizers.init(params) \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid optimizers. Expected `optax` optimizers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DefaultStepFn(StepFn):\n",
    "\n",
    "    @ft.partial(jax.jit, static_argnums=(0,))\n",
    "    def train_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        def loss_fn(params: hk.Params):\n",
    "            logits, new_state = self.transformed.apply(\n",
    "                params, state,\n",
    "                rng_key, # <== rng\n",
    "                inputs, is_training=True # <== inputs\n",
    "            )\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "            return (loss, new_state)\n",
    "        \n",
    "        inputs, labels = batch\n",
    "        rng_key, next_key = jrand.split(train_state.next_key)\n",
    "        state = train_state.state\n",
    "        (loss, new_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(train_state.params)\n",
    "        updates, new_opt_state = self.optimizers.update(\n",
    "            grads, train_state.opt_state, train_state.params)\n",
    "        new_params = optax.apply_updates(train_state.params, updates)\n",
    "        return TrainState(\n",
    "            epoch=train_state.epoch,\n",
    "            step=train_state.step + 1,\n",
    "            params=new_params,\n",
    "            state=new_state,\n",
    "            opt_state=new_opt_state,\n",
    "            next_key=next_key,\n",
    "            logs={'train/loss': loss}\n",
    "        )\n",
    "    \n",
    "    def val_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        inputs, labels = batch\n",
    "        rng_key, next_key = jrand.split(train_state.next_key)\n",
    "        logits, _ = self.transformed.apply(\n",
    "            train_state.params, train_state.state,\n",
    "            rng_key, # <== rng\n",
    "            inputs, is_training=False # <== inputs\n",
    "        )\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "        acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
    "        logs = {'val/loss': loss, \"val/accuracy\": acc}\n",
    "\n",
    "        return train_state._replace(\n",
    "            step=train_state.step + 1,\n",
    "            next_key=next_key, logs=logs\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBatchNorm(hk.Module):\n",
    "    \"Linear layer with batch normalization\"\n",
    "    def __init__(self, output_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, x, training=False):\n",
    "        return hk.BatchNorm(True, True, 0.9)(hk.Linear(self.output_size)(x), is_training=training)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hk_module(\n",
    "    module: hk.Module, # haiku module \n",
    "    *args, # haiku module arguments\n",
    "    **kargs, # haiku module arguments\n",
    ") -> hk.Transformed:\n",
    "\n",
    "    def model_fn(x, is_training: bool = True):\n",
    "        return module(*args, **kargs)(x, is_training)\n",
    "    \n",
    "    return hk.transform_with_state(model_fn)\n",
    "\n",
    "module = make_hk_module(LinearBatchNorm, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_dataloader import DataLoader, ArrayDataset\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = make_classification(n_samples=2000, n_features=10, random_state=0)\n",
    "ds = ArrayDataset(xs, ys)\n",
    "dl = DataLoader(ds, 'jax', batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    transformed=module,\n",
    "    optimizers=optax.adam(1e-3),\n",
    "    callbacks=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/miniconda3/envs/jax-dataloader/lib/python3.8/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
