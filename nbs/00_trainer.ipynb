{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import jax, jax.numpy as jnp, jax.random as jrand\n",
    "import haiku as hk\n",
    "import optax\n",
    "import chex\n",
    "from dataclasses import dataclass\n",
    "import functools as ft\n",
    "from typing import Callable, Tuple, Any, Sequence, Iterable, Mapping, Dict, List, NamedTuple\n",
    "import copy\n",
    "from haiku_trainer.callbacks import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainState(NamedTuple):\n",
    "    epoch: int\n",
    "    step: int\n",
    "    params: hk.Params\n",
    "    state: hk.State\n",
    "    opt_state: optax.OptState\n",
    "    next_key: jrand.PRNGKey\n",
    "    logs: dict = None\n",
    "\n",
    "    def __eq__(self, compare: TrainState) -> bool:\n",
    "        return (self.epoch == compare.epoch) and (self.step == compare.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Trainer:\n",
    "    transformed: hk.TransformedWithState | hk.MultiTransformedWithState\n",
    "    optimizers: optax.GradientTransformation | Sequence[optax.GradientTransformation]\n",
    "    rng_key: jrand.PRNGKey = None\n",
    "\n",
    "    # callback functions\n",
    "    callbacks: Sequence[Callback] = None\n",
    "    step_fn: StepFn = None\n",
    "\n",
    "    # trainer configs\n",
    "    lr: float = 1e-3\n",
    "    n_epochs: int = 1\n",
    "\n",
    "    @property\n",
    "    def train_state(self) -> TrainState:\n",
    "        \"\"\"Returns the current train state.\"\"\"\n",
    "        return self._train_state\n",
    "\n",
    "    @ft.cached_property\n",
    "    def num_train_batches(self) -> int:\n",
    "        \"\"\"Returns the number of training batches of each epoch.\"\"\"\n",
    "        loader = getattr(self, '_train_dataloader', None)\n",
    "        if loader is None:  return 0\n",
    "        else:               return len(loader)\n",
    "    \n",
    "    @property\n",
    "    def num_train_steps(self) -> int:\n",
    "        \"\"\"Returns the number of training steps.\"\"\"\n",
    "        return self.n_epochs * self.num_train_batches\n",
    "    \n",
    "    @property\n",
    "    def num_val_batches(self) -> int:\n",
    "        \"\"\"Returns the number of validation batches of each epoch.\"\"\"\n",
    "        loader = getattr(self, '_val_dataloader', None)\n",
    "        if loader is None:  return 0\n",
    "        else:               return len(loader)\n",
    "    \n",
    "    @property\n",
    "    def num_val_steps(self):\n",
    "        \"\"\"Returns the number of validation steps.\"\"\"\n",
    "        return self.n_epochs * self.num_val_batches\n",
    "\n",
    "    def _initialize_properties(self):\n",
    "        \"\"\"Initializes `train_state`.\"\"\"\n",
    "        if getattr(self, '_train_state', None) is None:\n",
    "            self._train_state = None\n",
    "    \n",
    "    def _initialize_key(self):\n",
    "        \"\"\"Initialize the `rng_key`.\"\"\"\n",
    "        if self.rng_key is None:    return jrand.PRNGKey(42) # TODO: use global\n",
    "        else:                       return self.rng_key\n",
    "\n",
    "    def _initialize_callbacks(self):\n",
    "        \"\"\"Initializes the callbacks.\"\"\"\n",
    "        if self.callbacks is None:\n",
    "            self.callbacks = CallbackList()\n",
    "        elif isinstance(self.callbacks, CallbackList):\n",
    "            self.callbacks = self.callbacks\n",
    "        elif isinstance(self.callbacks, Sequence):\n",
    "            self.callbacks = CallbackList(self.callbacks)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid callbacks. Expected `CallbackList` or `Sequence[Callback]`.\")\n",
    "\n",
    "        self.callbacks.init_trainer(self)\n",
    "\n",
    "    def _initialize_step_fn(self):\n",
    "        \"\"\"Initializes step fns.\"\"\"\n",
    "        if self.step_fn is None:\n",
    "            self.step_fn = DefaultStepFn(trainer=self)\n",
    "        else:\n",
    "            if isinstance(self.step_fn, StepFn):\n",
    "                self.step_fn.init_trainer(self)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid `Trainer.step_fn`. Expected `StepFn`, but got `{type(self.step_fn)}`.\")\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \"\"\"Initializes the trainer.\"\"\"\n",
    "        self._initialize_properties()\n",
    "        self._initialize_key()\n",
    "        self._initialize_callbacks()\n",
    "        self._initialize_step_fn()\n",
    "\n",
    "    def _update_loader(self, loader_name: str, loader=None):\n",
    "        if getattr(self, loader_name, None) is None:\n",
    "            setattr(self, loader_name, loader)\n",
    "        if loader is not None:\n",
    "            setattr(self, loader_name, loader)\n",
    "    \n",
    "    def _initialize_loaders(\n",
    "        self, \n",
    "        train_dataloader, \n",
    "        val_dataloader=None, \n",
    "        test_dataloader=None\n",
    "    ):\n",
    "        \"\"\"Initialize and hook dataloaders to the trainer.\"\"\"\n",
    "        self._update_loader('_train_dataloader', train_dataloader)\n",
    "        self._update_loader('_val_dataloader', val_dataloader)\n",
    "        self._update_loader('_test_dataloader', test_dataloader)\n",
    "        \n",
    "    def _run_callbacks(\n",
    "        self, \n",
    "        hook_name: str, # Should be \"on_{train/val}_{epoch/batch}_{begin/end}\"\n",
    "        **cb_kwargs # kwargs for the callback function\n",
    "    ):\n",
    "        \"\"\"Runs the callback functions for the given hook.\n",
    "        Note that callback functions do not change the `train_state`.\n",
    "        \"\"\"\n",
    "        hook_fn = getattr(self.callbacks, hook_name, None)\n",
    "        if hook_fn is not None:\n",
    "            hook_fn(self.train_state, **cb_kwargs)\n",
    "\n",
    "    def _run_step_fn(\n",
    "        self, \n",
    "        step_name: str, \n",
    "        validate: bool = False,\n",
    "        **fn_kwargs # kwargs for the step function\n",
    "    ):\n",
    "        \"\"\"Runs the step function for the given step name.\n",
    "        Note that step functions change the `train_state`.\n",
    "        \"\"\"\n",
    "        step_fn = getattr(self.step_fn, step_name)\n",
    "        train_state = step_fn(self.train_state, **fn_kwargs)\n",
    "\n",
    "        if validate and train_state == self.train_state:\n",
    "            raise ValueError(f\"Train state is not updated after `{step_name}`.\")\n",
    "        self.update_train_state(train_state)\n",
    "\n",
    "    def update_train_state(self, train_state: TrainState = None, **kwargs):\n",
    "        \"\"\"Updates the `train_state`.\"\"\"\n",
    "        if train_state is None and kwargs == {}:\n",
    "            raise ValueError(\"Either `train_state` or `kwargs` must be provided.\")\n",
    "        if train_state is None:\n",
    "            train_state = self.train_state._replace(**kwargs)\n",
    "        self._train_state = train_state\n",
    "\n",
    "    def fit(self, train_dataloader, val_dataloader=None):\n",
    "        self._initialize()\n",
    "        self._initialize_loaders(train_dataloader, val_dataloader)\n",
    "        self._run_callbacks(\"on_train_begin\")\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self._run_callbacks(\"on_epoch_begin\")\n",
    "            for batch in train_dataloader:\n",
    "                self._run_callbacks(\"on_train_batch_begin\")\n",
    "                # Initialize the train state if it is not initialized\n",
    "                if self.train_state is None:\n",
    "                    self._run_step_fn(\"init_step\", batch=batch)\n",
    "                self._run_step_fn(\"train_step\", batch=batch)\n",
    "                self._run_callbacks(\"on_train_batch_end\")\n",
    "            self._run_callbacks(\"on_epoch_end\")\n",
    "\n",
    "            if val_dataloader is not None:\n",
    "                self._run_callbacks(\"on_val_begin\")\n",
    "                for batch in val_dataloader:\n",
    "                    self._run_callbacks(\"on_val_batch_begin\")\n",
    "                    self._run_step_fn(\"val_step\", batch=batch)\n",
    "                    self._run_callbacks(\"on_val_batch_end\")\n",
    "                self._run_callbacks(\"on_val_end\")\n",
    "\n",
    "            self._run_callbacks(\"on_train_end\")\n",
    "            self._run_step_fn(\"epoch_step\", batch=None)\n",
    "        \n",
    "        self._run_callbacks(\"on_train_end\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StepFn:\n",
    "    def __init__(self, trainer: Trainer=None, *args, **kwargs) -> None:\n",
    "        if trainer is not None:\n",
    "            self.init_trainer(trainer)\n",
    "\n",
    "    def init_trainer(self, trainer: Trainer):\n",
    "        self._trainer = trainer\n",
    "\n",
    "    @property\n",
    "    def trainer(self): return self._trainer\n",
    "\n",
    "    @property\n",
    "    def transformed(self): return self.trainer.transformed\n",
    "\n",
    "    forward = transformed\n",
    "    \n",
    "    @property\n",
    "    def optimizers(self): return self.trainer.optimizers\n",
    "\n",
    "    def init_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        key1, next_key = jrand.split(self._init_key())\n",
    "        \n",
    "        params, state = self._init_params_and_state(key1, batch[0])\n",
    "        opt_states = self._init_opt_state(params)\n",
    "        return TrainState(\n",
    "            epoch=0, step=0, params=params, state=state, \n",
    "            opt_state=opt_states, next_key=next_key,\n",
    "        )\n",
    "    \n",
    "    def epoch_step(self, train_state: TrainState, batch=None) -> TrainState:\n",
    "        return train_state._replace(epoch=train_state.epoch+1)\n",
    "    \n",
    "    def train_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def val_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _init_key(self):\n",
    "        if self.trainer.rng_key is None:\n",
    "            return jrand.PRNGKey(0)\n",
    "        elif isinstance(self.trainer.rng_key, jrand.PRNGKey):\n",
    "            return self.trainer.rng_key\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid rng_key. Expected `jax.random.PRNGKey`.\")\n",
    "\n",
    "    def _init_params_and_state(self, key: jrand.PRNGKey, xs: jax.Array):\n",
    "        params, state = self.transformed.init(key, xs)\n",
    "        return params, state\n",
    "\n",
    "    def _init_opt_state(self, params: hk.Params):\n",
    "        if isinstance(self.optimizers, optax.GradientTransformation):\n",
    "            return self.optimizers.init(params) \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid optimizers. Expected `optax` optimizers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DefaultStepFn(StepFn):\n",
    "\n",
    "    @ft.partial(jax.jit, static_argnums=(0,))\n",
    "    def train_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        def loss_fn(params: hk.Params):\n",
    "            logits, new_state = self.transformed.apply(\n",
    "                params, state,\n",
    "                rng_key, # <== rng\n",
    "                inputs, is_training=True # <== inputs\n",
    "            )\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "            return (loss, new_state)\n",
    "        \n",
    "        inputs, labels = batch\n",
    "        rng_key, next_key = jrand.split(train_state.next_key)\n",
    "        state = train_state.state\n",
    "        (loss, new_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(train_state.params)\n",
    "        updates, new_opt_state = self.optimizers.update(\n",
    "            grads, train_state.opt_state, train_state.params)\n",
    "        new_params = optax.apply_updates(train_state.params, updates)\n",
    "        return TrainState(\n",
    "            epoch=train_state.epoch,\n",
    "            step=train_state.step + 1,\n",
    "            params=new_params,\n",
    "            state=new_state,\n",
    "            opt_state=new_opt_state,\n",
    "            next_key=next_key,\n",
    "            logs={'train/loss': loss}\n",
    "        )\n",
    "    \n",
    "    def val_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:\n",
    "        inputs, labels = batch\n",
    "        rng_key, next_key = jrand.split(train_state.next_key)\n",
    "        logits, _ = self.transformed.apply(\n",
    "            train_state.params, train_state.state,\n",
    "            rng_key, # <== rng\n",
    "            inputs, is_training=False # <== inputs\n",
    "        )\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "        acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
    "        logs = {'val/loss': loss, \"val/accuracy\": acc}\n",
    "\n",
    "        return train_state._replace(\n",
    "            step=train_state.step + 1,\n",
    "            next_key=next_key, logs=logs\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def make_hk_module(output_size: int = 2):\n",
    "    \"\"\"Creates a Haiku module with a linear layer and batchnorm.\"\"\"\n",
    "    def model(x, is_training=True):\n",
    "        return hk.BatchNorm(True, True, 0.9)(\n",
    "            hk.Linear(output_size)(x), is_training=is_training)\n",
    "    \n",
    "    return hk.transform_with_state(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = make_hk_module()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_dataloader import DataLoader, ArrayDataset\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = make_classification(n_samples=2000, n_features=10, random_state=0)\n",
    "ds = ArrayDataset(xs, ys)\n",
    "dl = DataLoader(ds, 'jax', batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    transformed=module,\n",
    "    optimizers=optax.adam(1e-3),\n",
    "    callbacks=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/dev/lib/python3.9/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trainer.num_train_batches == len(dl)\n",
    "assert trainer.num_train_steps == len(dl) * trainer.n_epochs\n",
    "assert trainer.num_val_batches == 0\n",
    "assert trainer.num_val_steps == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
