{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks.pbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from haiku_trainer.core import *\n",
    "import haiku as hk\n",
    "import importlib\n",
    "\n",
    "if importlib.util.find_spec(\"ipywidgets\") is not None:\n",
    "    from tqdm.auto import tqdm \n",
    "else:\n",
    "    from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _update_pbar_n(pbar: tqdm, n: int):\n",
    "    pbar.n = n\n",
    "    pbar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgbarLogger(Callback):\n",
    "    def __init__(self, ): \n",
    "        self._train_pbar = None\n",
    "        self._valid_pbar = None\n",
    "        self._batch_idx = None\n",
    "\n",
    "    @property\n",
    "    def train_pbar(self) -> tqdm: return self._train_pbar\n",
    "\n",
    "    @property\n",
    "    def valid_pbar(self) -> tqdm: return self._valid_pbar\n",
    "\n",
    "    @train_pbar.setter\n",
    "    def train_pbar(self, pbar: tqdm): self._train_pbar = pbar\n",
    "\n",
    "    @valid_pbar.setter\n",
    "    def valid_pbar(self, pbar: tqdm): self._valid_pbar = pbar\n",
    "    \n",
    "    @property\n",
    "    def num_train_batches(self) -> int: \n",
    "        return self.trainer.num_train_batches\n",
    "    \n",
    "    @property\n",
    "    def num_valid_batches(self) -> int:\n",
    "        return self.trainer.num_val_batches\n",
    "    \n",
    "    @property\n",
    "    def num_epoches(self) -> int:\n",
    "        return self.trainer.n_epochs\n",
    "\n",
    "    def init_train_pbar(self):\n",
    "        return tqdm(\n",
    "            desc='Training', leave=True, dynamic_ncols=True,\n",
    "            file=sys.stdout, smoothing=0, position=0\n",
    "        )\n",
    "    \n",
    "    def init_val_pbar(self):\n",
    "        return tqdm(\n",
    "            desc='Validation', leave=False, dynamic_ncols=True,\n",
    "            file=sys.stdout, smoothing=0, position=0\n",
    "        )\n",
    "    \n",
    "    def on_train_begin(self, state: TrainState):\n",
    "        self.train_pbar = self.init_train_pbar()\n",
    "\n",
    "    def on_epoch_begin(self, state: TrainState):\n",
    "        self.train_pbar.reset(self.num_train_batches)\n",
    "        self.train_pbar.initial = 0\n",
    "        self.train_pbar.set_description(f\"Epoch {state.epoch}\")\n",
    "        self._batch_idx = 0\n",
    "\n",
    "    def on_train_batch_end(self, state: TrainState):\n",
    "        self._batch_idx += 1\n",
    "        _update_pbar_n(self.train_pbar, self._batch_idx)\n",
    "        self.train_pbar.set_postfix(state.logs)\n",
    "        \n",
    "    def on_epoch_end(self, state: TrainState):\n",
    "        if not self.train_pbar.disable:\n",
    "            self.train_pbar.set_postfix(state.logs)\n",
    "\n",
    "    def on_train_end(self, state: TrainState):\n",
    "        self.train_pbar.close()\n",
    "\n",
    "    def on_val_begin(self, state: TrainState):\n",
    "        self.valid_pbar = self.init_val_pbar()\n",
    "        self.valid_pbar.reset(self.num_valid_batches)\n",
    "        self.valid_pbar.initial = 0\n",
    "        self.valid_pbar.set_description(f\"Validation Dataloader\")\n",
    "\n",
    "    def on_val_batch_end(self, state: TrainState):\n",
    "        self.valid_pbar.update(1)\n",
    "\n",
    "    def on_val_end(self, state: TrainState):\n",
    "        self.valid_pbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hk_module(output_size: int = 2):\n",
    "    \"\"\"Creates a Haiku module with a linear layer and batchnorm.\"\"\"\n",
    "    def model(x, is_training=True):\n",
    "        return hk.BatchNorm(True, True, 0.9)(\n",
    "            hk.Linear(output_size)(x), is_training=is_training)\n",
    "    \n",
    "    return hk.transform_with_state(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = make_hk_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_dataloader import DataLoader, ArrayDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = make_classification(n_samples=2000, n_features=10, random_state=0)\n",
    "train_xs, test_xs, train_ys, test_ys = train_test_split(xs, ys, test_size=0.2, random_state=0)\n",
    "train_ds = ArrayDataset(train_xs, train_ys)\n",
    "train_dl = DataLoader(train_ds, 'jax', batch_size=128)\n",
    "test_ds = ArrayDataset(test_xs, test_ys)\n",
    "test_dl = DataLoader(test_ds, 'jax', batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    transformed=module,\n",
    "    optimizers=optax.adam(1e-3),\n",
    "    callbacks=[ProgbarLogger()],\n",
    "    n_epochs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/dev/lib/python3.9/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac2ad95a4b44009a5653810c99f3074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2a0ec9123842f2b92e3f49f6b4d866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432800da04b0424784f5fd69ac1ef785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(train_dl, test_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
