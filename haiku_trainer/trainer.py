# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_trainer.ipynb.

# %% ../nbs/00_trainer.ipynb 2
from __future__ import annotations
import jax, jax.numpy as jnp, jax.random as jrand
import haiku as hk
import optax
import chex
from dataclasses import dataclass
import functools as ft
from typing import Callable, Tuple, Any, Sequence, Iterable, Mapping, Dict, List, NamedTuple
import copy

# %% auto 0
__all__ = ['TrainState', 'Trainer', 'StepFn', 'TrainStepFn', 'ValStepFn']

# %% ../nbs/00_trainer.ipynb 4
class TrainState(NamedTuple):
    epoch: int
    step: int
    params: hk.Params
    state: hk.State
    opt_state: optax.OptState
    next_key: jrand.PRNGKey
    logs: dict = None

# %% ../nbs/00_trainer.ipynb 5
@dataclass
class Trainer:
    transformed: hk.TransformedWithState | hk.MultiTransformedWithState
    optimizers: optax.GradientTransformation | Sequence[optax.GradientTransformation]
    rng_key: jrand.PRNGKey = None

    # callback functions
    callbacks: List[Any] = None
    train_step_fn: TrainStepFn = None
    val_step_fn: ValStepFn = None

    # trainer configs
    lr: float = 1e-3
    n_epochs: int = 1

    # model train state
    _train_state: TrainState = None

    @property
    def train_state(self):
        return self._train_state

    def _initialize_key(self):
        if self.rng_key is None:    return jrand.PRNGKey(42) # TODO: use global
        else:                       return self.rng_key

    def _initialize_model_param_and_state(
        self, key: jrand.PRNGKey, shape: Sequence[int]
    ):
        params, state = self.transformed.init(key, jnp.zeros(shape))
        return params, state

    def _initialize_opt_state(self, params: hk.Params):
        # if isinstance(self.optimizers, Sequence):
        #     return [opt.init(params) for opt in self.optimizers]
        if isinstance(self.optimizers, optax.GradientTransformation):
            return self.optimizers.init(params) 
        else:
            raise ValueError(f"Invalid optimizers. Expected `optax` optimizers.")
    
    # def _initialize_step_fns(self):
    #     if self.train_step_fn is None:
    #         self.train_step_fn = TrainStepFn(self)

    #     if self.val_step_fn is None:
    #         self.val_step_fn = ValStepFn(self)

    def _initialize(self, batch: Tuple[jax.Array, ...]):
        xs_shape = batch[0].shape
        key1, next_key = jrand.split(self._initialize_key())
        params, state = self._initialize_model_param_and_state(key1, shape=xs_shape)
        opt_states = self._initialize_opt_state(params)
        self._train_state = TrainState(
            epoch=0, step=0,
            params=params, state=state, 
            opt_state=opt_states,
            next_key=next_key,
        )
        # self._initialize_step_fns()
        
    def _run_callbacks(self):
        if self.callbacks is not None:
            for callback in self.callbacks:
                callback(self.train_state)

    def update_train_state(self, train_state: TrainState = None, **kwargs):
        if train_state is None and kwargs == {}:
            raise ValueError("Either `train_state` or `kwargs` must be provided.")
        if train_state is None:
            train_state = self.train_state._replace(**kwargs)
        self._train_state = train_state

    def train_step(self, batch: Tuple[jax.Array, ...]) -> None:
        if self.train_step_fn is None:
            self.train_step_fn = TrainStepFn(self)
        upt_train_state = self.train_step_fn(self.train_state, batch)
        self.update_train_state(upt_train_state)
    
    def val_step(self, batch: Tuple[jax.Array, ...]) -> None:
        if self.val_step_fn is None:
            self.val_step_fn = ValStepFn(self)
        upt_train_state = self.val_step_fn(self.train_state, batch)
        self.update_train_state(upt_train_state)

    def validate_train_step(self, batch: Tuple[jax.Array, ...]):
        cur_train_state = copy.deepcopy(self.train_state)
        self.train_step(batch)
        if cur_train_state.step == self.train_state.step:
            raise ValueError("Train state is not updated after `train_step`.")

    def fit(self, train_dataloader, val_dataloader=None):
        cur_steps = 0
        for epoch in range(self.n_epochs):
            for batch in train_dataloader:
                if self.train_state is None:
                    self._initialize(batch)
                    self.validate_train_step(batch)
                else:
                    self.train_step(batch)
                self._run_callbacks()
                cur_steps += 1
                assert cur_steps == self.train_state.step

            if val_dataloader is not None:
                for batch in val_dataloader:
                    self.val_step(self.train_state, batch)
                    cur_steps += 1
                    assert cur_steps == self.train_state.step

            self.update_train_state(epoch=epoch+1)


# %% ../nbs/00_trainer.ipynb 7
class StepFn:
    def __init__(self, trainer: Trainer, *args, **kwargs) -> None:
        self._trainer = trainer

    @property
    def trainer(self): return self._trainer

    @property
    def forward(self): return self.trainer.transformed
    
    @property
    def optimizer(self): return self.trainer.optimizers

# %% ../nbs/00_trainer.ipynb 8
class TrainStepFn(StepFn):
    loss_fn: Callable[[jax.Array, jax.Array], float] = None
    
    def __init__(self, trainer: Trainer, loss_fn=None) -> None:
        super().__init__(trainer)
        if loss_fn is None:
            loss_fn = optax.softmax_cross_entropy_with_integer_labels
        self.loss_fn = loss_fn

    @ft.partial(jax.jit, static_argnums=(0,))
    def __call__(self, train_state: TrainState, batch: Tuple[jax.Array, ...]):
        def loss_fn(params: hk.Params):
            logits, new_state = self.forward.apply(
                params, state,
                rng_key, # <== rng
                inputs, is_training=True # <== inputs
            )
            loss = self.loss_fn(logits, labels).mean()
            return (loss, new_state)
        
        inputs, labels = batch
        rng_key, next_key = jrand.split(train_state.next_key)
        state = train_state.state
        (loss, new_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(train_state.params)
        updates, new_opt_state = self.optimizer.update(
            grads, train_state.opt_state, train_state.params)
        new_params = optax.apply_updates(train_state.params, updates)
        return TrainState(
            epoch=train_state.epoch,
            step=train_state.step + 1,
            params=new_params,
            state=new_state,
            opt_state=new_opt_state,
            next_key=next_key,
            logs={'train/loss': loss}
        )

# %% ../nbs/00_trainer.ipynb 9
class ValStepFn(StepFn):
    metric_fns: Dict[str, Callable[[jax.Array, jax.Array], float]] = None

    def __init__(self, trainer: Trainer, metric_fns=None) -> None:
        super().__init__(trainer)
        if metric_fns is None:
            metric_fns = optax.softmax_cross_entropy_with_integer_labels
        self.metric_fns = metric_fns

    @ft.partial(jax.jit, static_argnums=(0,))
    def __call__(self, train_state: TrainState, batch: Tuple[jax.Array, ...]):        
        inputs, labels = batch
        rng_key, next_key = jrand.split(train_state.next_key)
        logits, _ = self.forward.apply(
            train_state.params, train_state.state,
            rng_key, # <== rng
            inputs, is_training=False # <== inputs
        )
        logs = {
            f'val/{name}': metric_fn(logits, labels)
            for name, metric_fn in self.metric_fns.items()
        }

        train_state = train_state._replace(
            step=train_state.step + 1,
            next_key=next_key, logs=logs
        )
        return train_state
